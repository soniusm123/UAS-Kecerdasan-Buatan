{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soniusm123/UAS-Kecerdasan-Buatan/blob/main/UAS_A_G231200015.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QqP4gnDZfmd"
      },
      "outputs": [],
      "source": [
        "#!pip install keras\n",
        "#Remove '#' in above line if not istalled keras\n",
        "import keras\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "dataset=load_iris()\n",
        "#How data looks like?\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Step : 1 : Know the data\n",
        "print(type(dataset))\n",
        "print(len(dataset))\n",
        "#Dataset is in bunch format. Bunch is a dictionary like object.\n",
        "print(dataset.keys())\n",
        "#We try to find out what values are stored in these attributes\n",
        "print(dataset[\"data\"][0:5])\n",
        "print(dataset[\"target\"][0:5])\n",
        "print(dataset[\"target_names\"][0:5])\n",
        "print(dataset[\"DESCR\"])\n",
        "print(dataset[\"feature_names\"])\n",
        "print(dataset[\"filename\"])\n",
        "#Next we define our x and y; y is the dependent on x\n",
        "X=dataset[\"data\"]\n",
        "print(len(X)) #To check length of dataset\n",
        "Y=dataset[\"target\"]\n",
        "print(len(Y)) #To check length of targets\n",
        "print(X, [0])\n",
        "print(Y, [0])"
      ],
      "metadata": {
        "id": "3dKXDFRYbP0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Step : 2 : covert y into one hot encoded vector\n",
        "#One hot encoding is used because Y has labels (0,1,2)- which is a categorical categorical data with no ordinal relationships\n",
        "from keras.utils import to_categorical\n",
        "Ny=len(np.unique(Y))\n",
        "print(Ny)\n",
        "Y = to_categorical(Y, num_classes = Ny)\n",
        "print(Y[0:5])"
      ],
      "metadata": {
        "id": "c3K3ObQhT2N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Step : 3 : Now we split the data into two parts - one for training another for testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, shuffle=True)\n",
        "print(x_train[0:5])\n",
        "print(x_test[0:5])\n",
        "print(y_train[0:5])\n",
        "print(y_test[0:5])"
      ],
      "metadata": {
        "id": "vPvmrDyoT2tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Step : 4 : Then we normalize the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#Normalization is done so that the difference between highest and lowest data point is not to large\n",
        "import numpy as np\n",
        "scaler=StandardScaler()\n",
        "##To find mean and std dev\n",
        "scaler.fit(x_train)\n",
        "\n",
        "##Converting data into form where mean of data is 0 and std dev is 1\n",
        "X_train=scaler.transform(x_train)\n",
        "X_test=scaler.transform(x_test)\n",
        "print(np.amax(X_train,axis=0))\n",
        "print(np.amin(X_train,axis=0))"
      ],
      "metadata": {
        "id": "5FWKmK_aUBXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Step : 5 : Now finallly we build the model using keras\n",
        "#!pip install tensorflow\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_dim=X_train.shape[1], activation='relu'))\n",
        "##Dropout is used to avoid overfitting\n",
        "keras.layers.Dropout(0.2)\n",
        "model.add(Dense(20, activation='relu'))\n",
        "keras.layers.Dropout(0.2)\n",
        "model.add(Dense(20, activation='relu'))\n",
        "keras.layers.Dropout(0.2)\n",
        "##For classification problems, we usually use softmax as activation function in final layer\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "##compiling the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "U7A-urDjUC3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Step : 6 : know the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "AwnwQ9-OUDso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Step : 7 : Train the model\n",
        "##Batch size, bepochs and validation split are all hyper parameters\n",
        "model.fit(X_train, y_train, epochs=195, batch_size=80, validation_split=0.1)"
      ],
      "metadata": {
        "id": "nmcusInBUEE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Step : 8 : Get the accuracy of model on testing data\n",
        "testing=model.evaluate(x_test, y_test)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1]+'uracy of Model on testing data', testing[1]*100))"
      ],
      "metadata": {
        "id": "5gf9RsZaUEwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Step : 9 : Evaluate our model\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "predictions = np.argmax(model.predict(X_test), axis=1)\n",
        "Y_test = np.argmax(y_test, axis=1)\n",
        "#To get confusion matrix\n",
        "print(confusion_matrix(Y_test, predictions))\n",
        "#To get values of all evaluation metrics\n",
        "print(classification_report(Y_test, predictions))"
      ],
      "metadata": {
        "id": "mBDkrh5KbPjx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}